{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/svetaepc/Python_START/blob/hw_work/%D0%B1%D1%8B%D1%81%D1%82%D1%80%D0%BE%D0%B5%20%D0%B2%D0%B2%D0%B5%D0%B4%D0%B5%D0%BD%D0%B8%D0%B5%20%D0%B2%20pandas/%D0%AD%D1%84%D1%84%D0%B5%D0%BA%D1%82%D0%B8%D0%B2%D0%BD%D0%B0%D1%8F%20%D0%BE%D1%87%D0%B8%D1%81%D1%82%D0%BA%D0%B0%20%D1%82%D0%B5%D0%BA%D1%81%D1%82%D0%B0%20%D1%81%20%D0%BF%D0%BE%D0%BC%D0%BE%D1%89%D1%8C%D1%8E%20Pandas.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LPi5BV5bsLN6"
      },
      "source": [
        "# Эффективная очистка текста с помощью Pandas\n",
        "\n",
        "<a href=\"https://t.me/init_python\"><img src=\"https://dfedorov.spb.ru/pandas/logo-telegram.png\" width=\"35\" height=\"35\" alt=\"telegram\" align=\"left\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tcvwof6zsLN8"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dm-fedorov/pandas_basic/blob/master/быстрое%20введение%20в%20pandas/Эффективная%20очистка%20текста%20с%20помощью%20Pandas.ipynb\" target=\"_blank\"><img align=\"left\" src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open in Colab\" title=\"Open and Execute in Google Colaboratory\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wBbtqvwxsLN9"
      },
      "source": [
        "## Вступление\n",
        "\n",
        "Очистка данных занимает значительную часть процесса анализа данных. При использовании *pandas* существует несколько методов очистки текстовых полей для подготовки к дальнейшему анализу. По мере того, как наборы данных увеличиваются, важно использовать эффективные методы.\n",
        "\n",
        "В этой статье будут показаны примеры очистки текстовых полей в большом файле и даны советы по эффективной очистке неструктурированных текстовых полей с помощью *Python* и *pandas*.\n",
        "\n",
        "> Оригинал статьи Криса по [ссылке](https://pbpython.com/text-cleaning.html)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "acza5sfwsLN9"
      },
      "source": [
        "## Проблема\n",
        "\n",
        "Предположим, что у вас есть новый крафтовый виски, который вы хотели бы продать. Ваша территория включает Айову, и там есть [открытый набор данных](https://data.iowa.gov/Sales-Distribution/Iowa-Liquor-Sales/m3tr-qhgy), который показывает продажи спиртных напитков в штате. Это кажется отличной возможностью, чтобы посмотреть, у кого самые большие счета в штате. Вооружившись этими данными, можно спланировать процесс продаж в магазины.\n",
        "\n",
        "В восторге от этой возможности, вы загружаете данные и понимаете, что они довольно большие. В этой статье я буду использовать данные, включающие продажи за `2019 год`. \n",
        "\n",
        "Выборочный набор данных представляет собой CSV-файл размером `565 МБ` с `24` столбцами и `2,3 млн` строк, а весь датасет занимает `5 Гб` (`25 млн` строк). Это ни в коем случае не большие данные, но они достаточно большие для обработки в *Excel* и некоторых методов *pandas*.\n",
        "\n",
        "Давайте начнем с импорта модулей и чтения данных. \n",
        "\n",
        "Я также воспользуюсь пакетом [`sidetable`](https://dfedorov.spb.ru/pandas/%D0%A1%D0%BE%D0%B7%D0%B4%D0%B0%D0%BD%D0%B8%D0%B5%20%D0%BF%D1%80%D0%BE%D1%81%D1%82%D1%8B%D1%85%20%D1%81%D0%B2%D0%BE%D0%B4%D0%BD%D1%8B%D1%85%20%D1%82%D0%B0%D0%B1%D0%BB%D0%B8%D1%86%20%D0%B2%20pandas%20%D1%81%20%D0%BF%D0%BE%D0%BC%D0%BE%D1%89%D1%8C%D1%8E%20sidetable.html) для обобщения данных. Он не требуется для очистки, но может быть полезен для подобных сценариев исследования данных."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c1lnikfksLN-"
      },
      "outputs": [],
      "source": [
        "#!pip3 install sidetable"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WGnXkyNTsLN_"
      },
      "source": [
        "## Данные\n",
        "\n",
        "Загрузим данные:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qqLjWeHtsLN_"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import sidetable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oLgcx8xlsLN_"
      },
      "outputs": [],
      "source": [
        "!wget https://www.dropbox.com/s/9e88whmc03nkouz/2019_Iowa_Liquor_Sales.csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_V3XW0z1sLN_"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('2019_Iowa_Liquor_Sales.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ly16YDX0sLOA"
      },
      "source": [
        "Посмотрим на них:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "osRwryaxsLOA"
      },
      "outputs": [],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zV0ynsoGsLOA"
      },
      "source": [
        "Первое, что можно сделать, это посмотреть, сколько закупает каждый магазин, и отсортировать их по убыванию. У нас ограниченные ресурсы, поэтому мы должны сосредоточиться на тех местах, где мы получим максимальную отдачу от вложенных средств. Нам будет проще позвонить паре крупных корпоративных клиентов, чем множеству семейных магазинов.\n",
        "\n",
        "Модуль [`sidetable`](https://dfedorov.spb.ru/pandas/%D0%A1%D0%BE%D0%B7%D0%B4%D0%B0%D0%BD%D0%B8%D0%B5%20%D0%BF%D1%80%D0%BE%D1%81%D1%82%D1%8B%D1%85%20%D1%81%D0%B2%D0%BE%D0%B4%D0%BD%D1%8B%D1%85%20%D1%82%D0%B0%D0%B1%D0%BB%D0%B8%D1%86%20%D0%B2%20pandas%20%D1%81%20%D0%BF%D0%BE%D0%BC%D0%BE%D1%89%D1%8C%D1%8E%20sidetable.html) позволяет обобщать данные в удобочитаемом формате и является альтернативой методу `groupby` с дополнительными преобразованиями."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "cv4yIjHksLOA"
      },
      "outputs": [],
      "source": [
        "df.stb.freq(['Store Name'], value='Sale (Dollars)', style=True, cum_cols=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6aQLxCaPsLOB"
      },
      "source": [
        "Похоже, во всех трех случаях  \n",
        "\n",
        "- `Hy-Vee #3 / BDI / Des Moines`\n",
        "- `Hy-Vee Wine and Spirits / Iowa City`\n",
        "- `Hy-Vee Food Store / Urbandale`\n",
        "\n",
        "речь идет об одном и том же магазине. Очевидно, что названия магазинов в большинстве случаев уникальны для каждого местоположения. \n",
        "\n",
        "В идеале мы хотели бы сгруппировать вместе все продажи `Hy-Vee`, `Costco` и т.д.\n",
        "\n",
        "Нам нужно очистить данные!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FEi5DJK_sLOB"
      },
      "source": [
        "### Попытка очистки №1\n",
        "\n",
        "Первый подход, который мы рассмотрим, - это использование `.loc` плюс логический фильтр с аксессором `str` для поиска соответствующей строки в столбце `Store Name`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z6AAEqjBsLOB"
      },
      "outputs": [],
      "source": [
        "df.loc[df['Store Name'].str.contains('Hy-Vee', case=False), 'Store_Group_1'] = 'Hy-Vee'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ms9juBosLOB"
      },
      "source": [
        "Этот код будет искать строку `Hy-Vee` без учета регистра и сохранять значение `Hy-Vee` в новом столбце с именем `Store_Group_1`. Данный код эффективно преобразует такие названия, как `Hy-Vee # 3 / BDI / Des Moines` или `Hy-Vee Food Store / Urbandale`, в обычное `Hy-Vee`.\n",
        "\n",
        "Вот, что `%timeit` говорит об эффективности:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rZoaK4AbsLOB"
      },
      "outputs": [],
      "source": [
        "%timeit df.loc[df['Store Name'].str.contains('Hy-Vee', case=False), 'Store_Group_1'] = 'Hy-Vee'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hwzmGxqGsLOC"
      },
      "source": [
        "Можем использовать параметр `regex=False` для ускорения вычислений:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lv8chs6usLOC"
      },
      "outputs": [],
      "source": [
        "%timeit df.loc[df['Store Name'].str.contains('Hy-Vee', case=False, regex=False), 'Store_Group_1'] = 'Hy-Vee'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X6Szc7PPsLOC"
      },
      "source": [
        "Вот значения в новом столбце:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gnxhXxhhsLOC"
      },
      "outputs": [],
      "source": [
        "df['Store_Group_1'].value_counts(dropna=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "REd_IIsbsLOC"
      },
      "source": [
        "Мы очистили `Hy-Vee`, но теперь появилось множество других значений, с которыми нам нужно разобраться.\n",
        "\n",
        "Подход `.loc` включает много кода и может быть медленным. Поищем альтернативы, которые быстрее выполнять и легче поддерживать."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mR9ApmtXsLOC"
      },
      "source": [
        "### Попытка очистки №2\n",
        "\n",
        "Другой очень эффективный и гибкий подход - использовать `np.select` для запуска нескольких совпадений и применения указанного значения при совпадении.\n",
        "\n",
        "Есть несколько хороших ресурсов, которые я использовал, чтобы узнать про `np.select`. Эта [статья](https://www.dataquest.io/blog/tutorial-add-column-pandas-dataframe-based-on-if-else-condition/) от *Dataquest* - хороший обзор, а также [презентация](https://docs.google.com/presentation/d/1X7CheRfv0n4_I21z4bivvsHt6IDxkuaiAuCclSzia1E/edit#slide=id.g635adc05c1_1_1840) Натана Чивера (*Nathan Cheever*). Рекомендую и то, и другое.\n",
        "\n",
        "Самое простое объяснение того, что делает `np.select`, состоит в том, что он оценивает список условий и применяет соответствующий список значений, если условие истинно.\n",
        "\n",
        "В нашем случае условиями будут разные строки для поиски (*string lookups*), а в качестве значений нормализованные строки, которые хотим использовать.\n",
        "\n",
        "После просмотра данных, вот список условий и значений в списке `store_patterns`. Каждый кортеж в этом списке представляет собой поиск по `str.contains()` и соответствующее текстовое значение, которое мы хотим использовать для группировки похожих счетов."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fHAG7olasLOD"
      },
      "outputs": [],
      "source": [
        "store_patterns = [\n",
        "    (df['Store Name'].str.contains('Hy-Vee', case=False, regex=False), 'Hy-Vee'),\n",
        "    (df['Store Name'].str.contains('Central City', case=False,  regex=False), 'Central City'),\n",
        "    (df['Store Name'].str.contains(\"Smokin' Joe's\", case=False,  regex=False), \"Smokin' Joe's\"),\n",
        "    (df['Store Name'].str.contains('Walmart|Wal-Mart', case=False), 'Wal-Mart'),\n",
        "    (df['Store Name'].str.contains('Fareway Stores', case=False,  regex=False), 'Fareway Stores'),\n",
        "    (df['Store Name'].str.contains(\"Casey's\", case=False,  regex=False), \"Casey's General Store\"),\n",
        "    (df['Store Name'].str.contains(\"Sam's Club\", case=False,  regex=False), \"Sam's Club\"),\n",
        "    (df['Store Name'].str.contains('Kum & Go', regex=False, case=False), 'Kum & Go'),\n",
        "    (df['Store Name'].str.contains('CVS', regex=False, case=False), 'CVS Pharmacy'),\n",
        "    (df['Store Name'].str.contains('Walgreens', regex=False, case=False), 'Walgreens'),\n",
        "    (df['Store Name'].str.contains('Yesway', regex=False, case=False), 'Yesway Store'),\n",
        "    (df['Store Name'].str.contains('Target Store', regex=False, case=False), 'Target'),\n",
        "    (df['Store Name'].str.contains('Quik Trip', regex=False, case=False), 'Quik Trip'),\n",
        "    (df['Store Name'].str.contains('Circle K', regex=False, case=False), 'Circle K'),\n",
        "    (df['Store Name'].str.contains('Hometown Foods', regex=False, case=False), 'Hometown Foods'),\n",
        "    (df['Store Name'].str.contains(\"Bucky's\", case=False, regex=False), \"Bucky's Express\"),\n",
        "    (df['Store Name'].str.contains('Kwik', case=False, regex=False), 'Kwik Shop')\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E3DYh613sLOD"
      },
      "source": [
        "Одна из серьезных проблем при работе с `np.select` заключается в том, что легко получить несоответствие условий и значений. Я решил объединить в кортеж, чтобы упростить отслеживание совпадений данных.\n",
        "\n",
        "Из-за такой структуры приходится разбивать список кортежей на два отдельных списка. \n",
        "\n",
        "Используя `zip`, можем взять `store_patterns` и разбить его на `store_criteria` и `store_values`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rEf26d1VsLOD"
      },
      "outputs": [],
      "source": [
        "store_criteria, store_values = zip(*store_patterns)\n",
        "df['Store_Group_1'] = np.select(store_criteria, store_values, 'other')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B_mxiWrTsLOD"
      },
      "source": [
        "Этот код будет заполнять каждое совпадение текстовым значением. Если совпадений нет, то присвоим ему значение `other`.\n",
        "\n",
        "Вот как это выглядит сейчас:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r7nurh2fsLOD"
      },
      "outputs": [],
      "source": [
        "df.stb.freq(['Store_Group_1'], value='Sale (Dollars)', style=True, cum_cols=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r84VhK6osLOE"
      },
      "source": [
        "Так лучше, но `32,28%` выручки по-прежнему приходится на `other` счета.\n",
        "\n",
        "Далее, если есть счет, который не соответствует шаблону, то используем `Store Name` вместо того, чтобы объединять все в `other`. \n",
        "\n",
        "Вот как мы это сделаем:`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yjPc9nPAsLOE"
      },
      "outputs": [],
      "source": [
        "df['Store_Group_1'] = np.select(store_criteria, store_values, None)\n",
        "df['Store_Group_1'] = df['Store_Group_1'].combine_first(df['Store Name'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VrSzVYLzsLOE"
      },
      "source": [
        "Здесь используется функция `comb_first`, чтобы заполнить все `None` значения `Store Name`. Это удобный прием, о котором следует помнить при очистке данных.\n",
        "\n",
        "Проверим наши данные:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "PO8Z1dt1sLOE"
      },
      "outputs": [],
      "source": [
        "df.stb.freq(['Store_Group_1'], value='Sale (Dollars)', style=True, cum_cols=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HaxciIWisLOE"
      },
      "source": [
        "Выглядит лучше, т.к. можем продолжать уточнять группировки по мере необходимости. Например, можно построить поиск по строке для `Costco`.\n",
        "\n",
        "Производительность не так уж и плоха для большого набора данных:\n",
        "\n",
        "    13.2 s ± 328 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
        "\n",
        "Гибкость данного подхода в том, что можно использовать `np.select` для числового анализа и текстовых примеров.\n",
        "\n",
        "Единственная проблема, связанная с этим подходом, заключается в большом количестве кода. \n",
        "\n",
        "Есть ли другой подход, который мог бы иметь аналогичную производительность, но был бы немного чище?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BUT7KqwnsLOF"
      },
      "source": [
        "### Попытка очистки №3\n",
        "\n",
        "Следующее решение основано на [этом](https://www.metasnake.com/blog/pydata-assign.html) примере кода от Мэтта Харрисона (*Matt Harrison*). Он разработал функцию `generalize`, которая выполняет сопоставление и очистку за нас! \n",
        "\n",
        "Я внес некоторые изменения, чтобы привести ее в соответствие с этим примером, но хочу отдать должное Мэтту. Я бы никогда не подумал об этом решении, если бы оно не выполняло `99%` всей работы!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RO15h25qsLOF"
      },
      "outputs": [],
      "source": [
        "def generalize(ser, match_name, default=None, regex=False, case=False):\n",
        "    \"\"\" Поиск в серии текстовых совпадений.\n",
        "    На основе кода из https://www.metasnake.com/blog/pydata-assign.html\n",
        "\n",
        "    ser: серии pandas для поиска \n",
        "    match_name: кортеж, содержащий текст для поиска и текст для нормализации\n",
        "    default: Если совпадений нет, используйте это, чтобы указать значение по умолчанию, \n",
        "    в противном случае используйте оригинальный текст\n",
        "    regex: Логическое значение, указывающее, содержит ли match_name регулярное выражение\n",
        "    case: Поиск с учетом регистра\n",
        "\n",
        "    Возвращает серию pandas с совпадающим значением\n",
        "    \"\"\"\n",
        "    seen = None\n",
        "    for match, name in match_name:\n",
        "        mask = ser.str.contains(match, case=case, regex=regex)\n",
        "        if seen is None:\n",
        "            seen = mask\n",
        "        else:\n",
        "            seen |= mask\n",
        "        ser = ser.where(~mask, name)\n",
        "    if default:\n",
        "        ser = ser.where(seen, default)\n",
        "    else:\n",
        "        ser = ser.where(seen, ser.values)\n",
        "    return ser"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ur30Z4q6sLOG"
      },
      "source": [
        "Эта функция может быть вызвана для серии *pandas* и ожидает список кортежей. \n",
        "\n",
        "Первый элемент следующего кортежа - это значение для поиска, а второй - значение, которое нужно заполнить для совпадающего значения.\n",
        "\n",
        "Вот список эквивалентных шаблонов:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jcxZMML5sLOG"
      },
      "outputs": [],
      "source": [
        "store_patterns_2 = [('Hy-Vee', 'Hy-Vee'), \n",
        "                    (\"Smokin' Joe's\", \"Smokin' Joe's\"),\n",
        "                    ('Central City', 'Central City'),\n",
        "                    ('Costco Wholesale', 'Costco Wholesale'),\n",
        "                    ('Walmart', 'Walmart'), \n",
        "                    ('Wal-Mart', 'Walmart'),\n",
        "                    ('Fareway Stores', 'Fareway Stores'),\n",
        "                    (\"Casey's\", \"Casey's General Store\"),\n",
        "                    (\"Sam's Club\", \"Sam's Club\"), \n",
        "                    ('Kum & Go', 'Kum & Go'),\n",
        "                    ('CVS', 'CVS Pharmacy'), \n",
        "                    ('Walgreens', 'Walgreens'),\n",
        "                    ('Yesway', 'Yesway Store'),\n",
        "                    ('Target Store', 'Target'),\n",
        "                    ('Quik Trip', 'Quik Trip'), \n",
        "                    ('Circle K', 'Circle K'),\n",
        "                    ('Hometown Foods', 'Hometown Foods'),\n",
        "                    (\"Bucky's\", \"Bucky's Express\"), \n",
        "                    ('Kwik', 'Kwik Shop')]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8m8EyF4gsLOG"
      },
      "source": [
        "Преимущество этого решения состоит в том, что поддерживать данный список намного проще, чем в предыдущем примере `store_patterns`.\n",
        "\n",
        "Другое изменение, которое я внес с помощью функции `generalize`, заключается в том, что исходное значение будет сохранено, если не указано значение по умолчанию. Теперь вместо использования `combine_first` функция `generalize` позаботится обо всем. \n",
        "\n",
        "Наконец, я отключил сопоставление регулярных выражений по умолчанию для улучшения производительности.\n",
        "\n",
        "Теперь, когда все данные настроены, вызвать их очень просто:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RQ0hwTyDsLOG"
      },
      "outputs": [],
      "source": [
        "df['Store_Group_2'] = generalize(df['Store Name'], store_patterns_2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cvcQLfE7sLOH"
      },
      "source": [
        "Как насчет производительности?\n",
        "\n",
        "    15.5 s ± 409 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
        "\n",
        "Немного медленнее, но думаю, что это более элегантное решение и я бы использовал его в будущем.\n",
        "\n",
        "Обратной стороной этого подхода является то, что он предназначен для очистки строк. Решение `np.select` более полезно, поскольку его можно применять и к числовым значениям.\n",
        "\n",
        "### А как насчет типов данных?\n",
        "\n",
        "В последних версиях *pandas* есть специальный тип `string`. Я попытался преобразовать `Store Name` в строковый тип *pandas*, чтобы увидеть, есть ли улучшение производительности. Никаких изменений не заметил. Однако не исключено, что в будущем скорость будет повышена, так что имейте это в виду.\n",
        "\n",
        "Тип `category` показал многообещающие результаты. \n",
        "\n",
        "> Обратитесь к моей [предыдущей статье](https://dfedorov.spb.ru/pandas/%D0%98%D1%81%D0%BF%D0%BE%D0%BB%D1%8C%D0%B7%D0%BE%D0%B2%D0%B0%D0%BD%D0%B8%D0%B5%20%D1%82%D0%B8%D0%BF%D0%B0%20%D0%B4%D0%B0%D0%BD%D0%BD%D1%8B%D1%85%20%D0%BA%D0%B0%D1%82%D0%B5%D0%B3%D0%BE%D1%80%D0%B8%D0%B8%20%D0%B2%20pandas.html) за подробностями о типе данных категории.\n",
        "\n",
        "Можем преобразовать данные в тип `category` с помощью `astype`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5QUtkQ6UsLOH"
      },
      "outputs": [],
      "source": [
        "df['Store Name'] = df['Store Name'].astype('category')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kZzQj9iWsLOH"
      },
      "source": [
        "Теперь повторно запустите пример `np.select` точно так же, как мы делали ранее:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pjDZMvjFsLOH"
      },
      "outputs": [],
      "source": [
        "df['Store_Group_3'] = np.select(store_criteria, store_values, None)\n",
        "df['Store_Group_3'] = df['Store_Group_1'].combine_first(df['Store Name'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rNeK7nPwsLOI"
      },
      "source": [
        "    786 ms ± 108 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
        "\n",
        "Мы перешли с `13` до менее `1 секунды`, сделав одно простое изменение. Удивительно!\n",
        "\n",
        "Причина, по которой это произошло, довольно проста. Когда *pandas* преобразует столбец в категориальный тип, функция `str.contains()` будет вызываться для каждого уникального текстового значения. Поскольку этот набор данных содержит много повторяющихся данных, мы получаем огромный прирост производительности.\n",
        "\n",
        "Посмотрим, работает ли это для нашей функции `generalize`:\n",
        "\n",
        "    df['Store_Group_4'] = generalize(df['Store Name'], store_patterns_2)\n",
        "\n",
        "К сожалению, получаем ошибку:\n",
        "\n",
        "    ValueError: Cannot setitem on a Categorical with a new category, set the categories first\n",
        "\n",
        "Эта ошибка подчеркивает некоторые проблемы, с которыми я сталкивался в прошлом при работе с категориальными (*Categorical*) данными. При *merging* и *joining* категориальных данных вы можете столкнуться с подобными типами проблем.\n",
        "\n",
        "Я попытался найти хороший способ изменить работу `generalize()`, но не смог. \n",
        "\n",
        "Тем не менее есть способ воспроизвести категориальный подход (*Category approach*), построив [таблицу поиска](https://ru.wikipedia.org/wiki/%D0%A2%D0%B0%D0%B1%D0%BB%D0%B8%D1%86%D0%B0_%D0%BF%D0%BE%D0%B8%D1%81%D0%BA%D0%B0) (*lookup table*)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SlqxE5rasLOI"
      },
      "source": [
        "### Таблица поиска\n",
        "\n",
        "Как мы узнали из категориального подхода, данный набор содержит много повторяющихся данных. \n",
        "\n",
        "Мы можем построить таблицу поиска и запустить ресурсоемкую функцию только один раз для каждой строки.\n",
        "\n",
        "Чтобы проиллюстрировать, как это работает со строками, давайте преобразуем значение обратно в строковый тип вместо категории:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fruf9frLsLOI"
      },
      "outputs": [],
      "source": [
        "df['Store Name'] = df['Store Name'].astype('string')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rOgYjGBTsLOI"
      },
      "outputs": [],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TNVt0NLMsLOJ"
      },
      "source": [
        "Сначала мы создаем `DataFrame` поиска, который содержит все уникальные значения, и запускаем функцию `generalize`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8sSX1bGCsLOJ"
      },
      "outputs": [],
      "source": [
        "lookup_df = pd.DataFrame()\n",
        "lookup_df['Store Name'] = df['Store Name'].unique()\n",
        "lookup_df['Store_Group_5'] = generalize(lookup_df['Store Name'], store_patterns_2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bxqn_yQmsLOJ"
      },
      "outputs": [],
      "source": [
        "lookup_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ynkaZ9UjsLOJ"
      },
      "source": [
        "Можем объединить (*merge*) его обратно в окончательный `DataFrame`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6DwqUUOLsLOJ"
      },
      "outputs": [],
      "source": [
        "df = pd.merge(df, lookup_df, how='left')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gC0nED9usLOK"
      },
      "source": [
        "    1.38 s ± 15.1 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
        "\n",
        "Он работает медленнее, чем подход `np.select` для категориальных данных, но влияние на производительность может быть уравновешено более простой читабельностью для ведения списка поиска.\n",
        "\n",
        "Кроме того, промежуточный `lookup_df` может стать отличным выходом для аналитика, который поможет очистить больше данных. Эту экономию можно измерить часами работы!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PAQzujcfsLOK"
      },
      "source": [
        "## Резюме\n",
        "\n",
        "[Этот](https://counting.substack.com/p/data-cleaning-is-analysis-not-grunt) информационный бюллетень Рэнди Ау (*Randy Au*) - хорошее обсуждение важности очистки данных и отношения любви / ненависти, которое многие специалисты по данным чувствуют при выполнении данной задачи. Я согласен с предположением Рэнди о том, что очистка данных - это анализ.\n",
        "\n",
        "По моему опыту, вы можете многое узнать о своих базовых данных, взяв на себя действия по очистке, описанные в этой статье.\n",
        "\n",
        "Я подозреваю, что в ходе повседневного анализа вы найдете множество случаев, когда вам нужно очистить текст, аналогично тому, что я показал выше.\n",
        "\n",
        "Вот краткое изложение рассмотренных решений:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SCz0RQmVsLOK"
      },
      "source": [
        "|Решение   |Время исполнения   |Примечания   |\n",
        "|---|---|---|\n",
        "|`np.select`   | `13 с` |Может работать для нетекстового анализа   |\n",
        "|`generalize`  | `15 с` |Только текст   |\n",
        "|Категориальные данные и `np.select`   |`786 мс`  |Категориальные данные могут быть сложными при *merging* и *joining*   |\n",
        "|Таблица поиска и `generalize`   | `1.3 с` |Таблица поиска может поддерживаться кем-то другим|"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9m_PCjzxsLOK"
      },
      "source": [
        "Для некоторых наборов данных производительность не является проблемой, поэтому выбирайте то, что вам ближе.\n",
        "\n",
        "Однако по мере увеличения размера данных (представьте, что вы проводите анализ для `50` штатов), вам нужно будет понять, как эффективно использовать *pandas* для очистки текста. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AitjyGMMsLOK"
      },
      "source": [
        "<a href=\"https://t.me/init_python\"><img src=\"https://dfedorov.spb.ru/pandas/logo-telegram.png\" width=\"35\" height=\"35\" alt=\"telegram\" align=\"left\"></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RKZe4DhfsLOK"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}